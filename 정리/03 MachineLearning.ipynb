{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os name: nt\n",
      "python ver: 3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]\n",
      "numpy ver: 1.23.4\n",
      "pandas ver: 1.5.0\n",
      "matplotlib ver: 3.6.0\n",
      "seaborn ver: 0.12.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print('os name:', os.name)\n",
    "print('python ver:', sys.version)\n",
    "print('numpy ver:', np.__version__)\n",
    "print('pandas ver:', pd.__version__)\n",
    "print('matplotlib ver:', matplotlib.__version__)\n",
    "print('seaborn ver:', sns.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.min_rows', 50)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 가공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통된 컬럼 찾기\n",
    "set(df_y.columns)&set(df_x.columns)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target 균등 분포 확인(특히 분류 분석 시)\n",
    "value_counts(normalize=True)\n",
    "\n",
    "# Correlation 확인 (높은 상관관계를 갖는 독립변수를 찾아 제거 or 축소(PCA: 주성분분석) 하기 위해)\n",
    "# 제거 시에는 짝이 되는 두 컬럼 중 Target과 Corr이 더 작은 COL을 삭제\n",
    "df.corr(numeric_only=True).round(3)\n",
    "sns.heatmap(df.corr(numeric_only=True))\n",
    "\n",
    "for idx in range(len(df.corr())):\n",
    "    s = df.corr().iloc[idx]\n",
    "    print(s.drop(s.index[idx]).max())\n",
    "\n",
    "df.iloc[:, :-1].corr()  # 가장 오른쪽에 있는 Target값의 dtype이 object인 경우 제외시키기 위해\n",
    "\n",
    "# 독립변수와 종속변수 간 선형성 확인을 위해\n",
    "B= df.corr(numeric_only=True)['Y']\n",
    "B = B.loc[B.index!='Y']\n",
    "B\n",
    "\n",
    "df.loc[:,df.columns != 'Y'].corrwith(df['Y']) #특정 컬럼 제외 후 그 컬럼에 대해 corr 산출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistically Significant가 나오면 변수로 넣어주자\n",
    "\n",
    "##### 범주형 - 범주형 두개의 변수는 독립일까? 관계가 있을까?\n",
    "##### 카이제곱 검정\n",
    "##### 귀무가설 : 두 변수의 관계는 독립이다\n",
    "##### 대립가설 : 두 변수의 관계는 독립이 아니다\n",
    "##### p ≤ 0.05 일때 두 변수는 독립이 아닌 관계가 있는 것\n",
    "from scipy import stats\n",
    "\n",
    "def statistical_test(mode, df, cat, target):\n",
    "    uniques= df[cat].unique()    \n",
    "    if mode == 't':   # 범주가 1개, 2개 일때 사용 - 범주별 평균의 차이가 유의미한가? (성별에 따른 키 평균)\n",
    "        group = df.groupby(cat)[target]\n",
    "        samples = [group.get_group(i) for i in uniques]\n",
    "        value, p = stats.ttest_ind(*samples)        \n",
    "    elif mode == 'f':   # 범주가 3개 이상일 때 사용 - 범주별 평균의 차이가 유의미한가? (혈액형에 따른 키 평균)\n",
    "        group = df.groupby(cat)[target]\n",
    "        samples = [group.get_group(i) for i in uniques]\n",
    "        value, p = stats.f_oneway(*samples)\n",
    "    elif mode == 'c':   # 두 개 범주가 독립성을 갖는지 아닌지? (가사노동의 종류와 구성원은 독립적? 연관성?)\n",
    "        contingency = pd.crosstab(index=df[cat], columns=df[target])\n",
    "        value, p, df, expected = stats.chi2_contingency(contingency)\n",
    "        \n",
    "    return value, p, 'Statistically Signifcant' if p<=0.05 else 'Statistically Insignifcant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding - 명목형 데이터와 더 잘 어울림\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "# Label Encoding - 명목형 데이터(성별, 혈액형, 국가, 지역, 부서)에 사용은 맞지 않음. (왜냐하면 숫자로 바꿔버리면 숫자의 크기가 생겨버려서)\n",
    "# Label Encoding을 명목형 데이터에 쓰면 오히려 안 좋아지는 결과 나올 수 있음\n",
    "LabelEncoder().fit_transform()  #preprocessing 모듈 호출 필요\n",
    "\n",
    "# s.astype('category').cat.codes 사용법\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Series name: None\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "10 non-null     int8 \n",
      "dtypes: int8(1)\n",
      "memory usage: 138.0 bytes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    2\n",
       "3    0\n",
       "4    0\n",
       "5    1\n",
       "6    0\n",
       "7    1\n",
       "8    2\n",
       "9    2\n",
       "dtype: int8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series(list(\"ABCAABABCC\"))\n",
    "s = s.astype('category').cat.codes  #눈에 보일때는 문자열이지만 실제 저장된 것은 숫자\n",
    "s.info(memory_usage='deep')\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Series name: None\n",
      "Non-Null Count  Dtype   \n",
      "--------------  -----   \n",
      "10 non-null     category\n",
      "dtypes: category(1)\n",
      "memory usage: 270.0 bytes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    2\n",
       "3    0\n",
       "4    0\n",
       "5    1\n",
       "6    0\n",
       "7    1\n",
       "8    2\n",
       "9    2\n",
       "dtype: category\n",
       "Categories (3, int64): [0, 1, 2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = s.astype('category') #이걸 쓰면 안에 data set 수정은 못하지만 메모리를 더 적게 씀\n",
    "s.info(memory_usage='deep')\n",
    "s\n",
    "#s[1]=\"D\" #Category로 만들어 버리면 새로 data를 추가 못함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "Y = df.iloc[:, -1]\n",
    "print(X.shape, Y.shape)  # Row 개수가 동일하지 확인 (Row, Col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스케일링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "degree = 3\n",
    "P = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "X_poly = P.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "- test_size = 0.25: 0.0~1.0 테스트 데이터셋 비율\n",
    "- train_size = None: 0.0~1.0 훈련 데이터셋 비율, test_size를 작성하면 작성하지 않아도 자동 산출\n",
    "- random_state = None: 정수 값, 난수 발생의 시드(seed) 값\n",
    "- stratify: y의 지정한 데이터 비율을 유지(층화추출), y가 범주형일 때 사용함\n",
    "\n",
    "A = train_test_split(X, Y, random_state=0, test_size=0.25, stratify=y)  # Scaler 미사용 시\n",
    "A = train_test_split(X_scaled, Y, random_state=0, test_size=0.25, stratify=y)  # Scaler 사용 시 X_scaled 확인 필요\n",
    "A = train_test_split(X_poly, Y, random_state=0, test_size=0.25)  # PolynomialFeatures 사용 시 X_scaled 확인 필요\n",
    "\n",
    "x_train, x_test, y_train, y_test = A\n",
    "\n",
    "[x.shape for x in [x_train, x_test, y_train, y_test]]  #데이터 분할된 비율이 제대로 됐는지 확인 목적\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형 모델링(분류-범주형)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_iter = 100: Maximum number of iterations taken for the solvers to converge. Tolerance값보다 작으면 max_iter 전에 빠져나옴\n",
    "Undefitting일 경우 크게, overfitting일 경우 작게 설정\n",
    "# tol = 0.0001: Tolerance for stopping criteria.\n",
    "Overfitting일 경우 크게 설정\n",
    "#C =1: Inverse of regularization strength; must be a positive float, 규제 강도인 alpha와 반대 개념\n",
    "Overfitting일 경우 작게 설정\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(penalty='l2', C=1.0, tol=0.0001, random_state=None, max_iter=100).fit(x_train,y_train)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_gini = DecisionTreeClassifier(criterion='gini', random_state=0).fit(x_train, y_train)\n",
    "tree_ent = DecisionTreeClassifier(criterion='entropy', random_state=0).fit(x_train, y_train)\n",
    "  # criterion : 'gini'(default), 'entropy'\n",
    "  # max_depth = None, 트리의 최대 깊이\n",
    "  # min_sample_split = 2, 노드 내에서 분할이 필요한 최소의 샘플 수\n",
    "  # max_features = None, 최적의 분할을 찾기 위해 고려할 Feature의 수\n",
    "  #  - int : Feature의 수, float : 비율, or {'auto', 'sqrt', 'log2'}\n",
    "  # max_leaf_nodes=None : 최대 말단 노드 개수\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형 모델링(회귀-연속형)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression  # y1 = a*x1 + b\n",
    "\n",
    "# Regularization(정칙화) : Overfitting을 해결하기 위해 학습 시 규제 강도 설정\n",
    "# alpha(규제 강도): Overfitting 발생 시 모델을 단순화(=규제) 시키기 위한 정도, alpha가 너무 크면 과소적합, 너무 작으면 과대적합\n",
    "\n",
    "from sklearn.linear_model import Lasso  # L1, 가중치의 절댓값의 합이 최소가 되도록 설정, alpha가 커질수록 사용되는 X 종류가 줄어듬, 사전에 Scaling 필요\n",
    "from sklearn.linear_model import Ridge  # L2, 가중치의 제곱합이 최소가 되도록 설정, 특정 가중치로 쏠리는 형상 방지, 사전에 Scaling 필요\n",
    "\n",
    "alpha = 1 \n",
    "model = Ridge(alpha=alpha).fit(x_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비선형 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x_train, y_train)\n",
    "model.score(x_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#실제값\n",
    "y_test\n",
    "\n",
    "#예측값\n",
    "model.predict(x_test)\n",
    "\n",
    "# 처음 5개만 Test 하고 싶을 때\n",
    "y_test.iloc[:5].to_numpy() #실제값(0~4번째 컬럼)\n",
    "model.predict(x_test[:5]) #에측값\n",
    "\n",
    "# 분류 모델일 경우, 범주 별 예측 확률을 출력해주며 가장 높은 확률을 가진 것이 모델의 예측값으로 출력됨\n",
    "model.predict_proba(x_test)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def print_lr_errors(model, X_test, y_test):\n",
    "    pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    rmse = np.sqrt(mse)  # = mse**0.5\n",
    "    r2 = r2_score(y_test, pred)\n",
    "    print(f'MAE:{mae:.3f}, MSE:{mse:.3f}, RMSE:{rmse:.3f}, R2:{r2:.3f}')\n",
    "\n",
    "\n",
    "print_lr_errors(model, x_train, y_train)\n",
    "print_lr_errors(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=10)\n",
    "#모델링\n",
    "def modeling(model,x_train,x_test,y_train,y_test):\n",
    "    model.fit(x_train,y_train)\n",
    "    pred = model.predict(x_test)\n",
    "    metrics(y_test,pred)\n",
    "#평가 지표\n",
    "def metrics(y_test,pred):\n",
    "    accuracy = accuracy_score(y_test,pred)\n",
    "    precision = precision_score(y_test,pred)\n",
    "    recall = recall_score(y_test,pred)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    roc_score = roc_auc_score(y_test,pred,average='macro')\n",
    "    print('정확도 : {0:.2f}, 정밀도 : {1:.2f}, 재현율 : {2:.2f}'.format(accuracy,precision,recall))\n",
    "    print('f1-score : {0:.2f}, auc : {1:.2f}'.format(f1,roc_score,recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
